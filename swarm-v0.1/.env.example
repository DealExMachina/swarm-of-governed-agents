# S3-compatible endpoint (RustFS)
S3_ENDPOINT=http://localhost:9000
S3_REGION=us-east-1
S3_ACCESS_KEY=minioadmin
S3_SECRET_KEY=minioadmin
S3_BUCKET=swarm

# Polling interval
TICK_SECONDS=10

# Facts worker (Python DSPy/RLM service that does the actual LLM extraction)
FACTS_WORKER_URL=http://localhost:8010
# Set to 1 to use Mastra LLM Agent for tool orchestration instead of the direct pipeline.
# Default (unset or 0): direct pipeline calls readContext -> worker /extract -> writeFacts.
# FACTS_USE_MASTRA=0

# OpenAI-compatible endpoint (worker + optional Mastra agent). Put real key only in .env (gitignored).
# OPENAI_BASE_URL is always passed to Mastra so it uses chat/completions (not the Responses API).
OPENAI_API_KEY=sk-xxxx
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini

# Postgres (local dev, port 5433 to avoid conflict with other instances)
POSTGRES_USER=swarm
POSTGRES_PASSWORD=swarm
POSTGRES_DB=swarm
DATABASE_URL=postgresql://swarm:swarm@localhost:5433/swarm

# NATS JetStream (event bus)
NATS_URL=nats://localhost:4222
NATS_STREAM=SWARM_JOBS

# Swarm agent identity
AGENT_ID=agent-1
AGENT_ROLE=facts

# Optional: bootstrap jobs on first run
# BOOTSTRAP=1

# Optional: governance config path
# GOVERNANCE_PATH=./governance.yaml

# OpenFGA (policy check). HTTP API on 8080; Playground at http://localhost:3000/playground (port 3000 root / returns 404)
# OPENFGA_URL=http://localhost:8080
# OPENFGA_STORE_ID=
# OPENFGA_MODEL_ID=
# OPENFGA_ALLOW_IF_UNAVAILABLE=1

# MITL approval server port (governance agent)
# MITL_PORT=3001

# Agent log files (swarm-all.sh). Default /tmp; logs written as swarm-<role>.log.
# LOG_DIR=/tmp

# OpenTelemetry (optional). When set, traces/metrics are sent to the collector.
# OTEL_SERVICE_NAME=swarm-v0.1
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# OTEL_SDK_DISABLED=0

# Phoenix LLM tracing (optional). Configure in facts-worker or set PHOENIX_COLLECTOR_ENDPOINT for OpenInference.
# PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006
