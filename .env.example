# S3-compatible endpoint (RustFS)
# Use 127.0.0.1 (not localhost) — Node 18 fetch resolves localhost to IPv6 ::1; Docker/Ollama may bind IPv4 only.
S3_ENDPOINT=http://127.0.0.1:9000
S3_REGION=us-east-1
S3_ACCESS_KEY=changeme
S3_SECRET_KEY=changeme
S3_BUCKET=swarm

# Polling interval
TICK_SECONDS=10

# Facts worker (Python DSPy/RLM service that does the actual LLM extraction)
FACTS_WORKER_URL=http://127.0.0.1:8010
# Set to 1 to use Mastra LLM Agent for tool orchestration instead of the direct pipeline.
# Default (unset or 0): direct pipeline calls readContext -> worker /extract -> writeFacts.
# FACTS_USE_MASTRA=0
# Set to 1 to embed claim nodes (Ollama bge-m3) after syncing facts to the semantic graph.
# FACTS_SYNC_EMBED=0

# OpenAI-compatible endpoint (worker + optional Mastra agent). Put real key only in .env (gitignored).
# OPENAI_BASE_URL is always passed to Mastra so it uses chat/completions (not the Responses API).
OPENAI_API_KEY=sk-xxxx
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4o-mini
# Optional: smaller/cheaper model for the oversight (routing) agent; defaults to OPENAI_MODEL.
# OVERSEE_MODEL=gpt-4o-mini

# Postgres (local dev, port 5433 to avoid conflict with other instances)
POSTGRES_USER=swarm
POSTGRES_PASSWORD=changeme
POSTGRES_DB=swarm
DATABASE_URL=postgresql://swarm:changeme@localhost:5433/swarm

# NATS JetStream (event bus)
NATS_URL=nats://localhost:4222
NATS_STREAM=SWARM_JOBS

# Scope id for finality and multi-tenant readiness (v0.1 single-scope default)
SCOPE_ID=default

# Swarm agent identity
AGENT_ID=agent-1
AGENT_ROLE=facts

# Optional: bootstrap jobs on first run
# BOOTSTRAP=1

# Optional: governance config path
# GOVERNANCE_PATH=./governance.yaml

# OpenFGA (policy check). HTTP API on 8080; Playground at http://localhost:3000/playground (port 3000 root / returns 404)
# OPENFGA_URL=http://localhost:8080
# OPENFGA_STORE_ID=
# OPENFGA_MODEL_ID=
# OPENFGA_ALLOW_IF_UNAVAILABLE=1

# MITL approval server (governance agent). Feed uses this to proxy GET /pending and POST /finality-response.
# MITL_PORT=3001
# MITL_URL=http://localhost:3001

# Agent log files (swarm-all.sh). Default /tmp; logs written as swarm-<role>.log.
# LOG_DIR=/tmp

# OpenTelemetry (optional). When set, traces/metrics are sent to the collector.
# OTEL_SERVICE_NAME=swarm-v0.1
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
# OTEL_SDK_DISABLED=0

# Phoenix LLM tracing (optional). Configure in facts-worker or set PHOENIX_COLLECTOR_ENDPOINT for OpenInference.
# PHOENIX_COLLECTOR_ENDPOINT=http://localhost:6006

# Ollama (local LLM). When set, facts-worker and TypeScript use Ollama instead of OpenAI for extraction/rationale/HITL/embeddings.
# Use 127.0.0.1 (not localhost) — Node 18 fetch resolves localhost to IPv6 ::1 first; Ollama binds IPv4 only.
# docker-compose.yml auto-overrides to host.docker.internal for containers.
OLLAMA_BASE_URL=http://127.0.0.1:11434
EXTRACTION_MODEL=qwen3:8b
RATIONALE_MODEL=phi4-mini
HITL_MODEL=mistral-small:22b
EMBEDDING_MODEL=bge-m3

# HuggingFace (Python worker: GLiNER2 NER, NLI). Put real token only in .env (gitignored).
# HF_TOKEN=hf_...
# GLINER_MODEL=fastino/gliner2-base-v1
# NLI_MODEL=cross-encoder/nli-deberta-v3-small

# Finality thresholds (goal gradient HITL vs automatic RESOLVED)
# For the demo scenario, 0.40 ensures HITL triggers after 5 docs (goal_score ~0.45-0.55)
NEAR_FINALITY_THRESHOLD=0.40
AUTO_FINALITY_THRESHOLD=0.92
